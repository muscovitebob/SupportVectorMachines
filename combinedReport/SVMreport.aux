\relax 
\citation{pelckmansLSSVMlabMATLABToolbox}
\citation{valyonRobustLSSVMRegression2007}
\citation{valyonRobustLSSVMRegression2007}
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise I: Basic support vector machines}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Exercise II: Function estimation and time series prediction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Support vector machine for function estimation}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A linear least squares regression fitted to a data cloud of 43 points with parameters $C=0.5$ and $e=0.1$. Given these parameters, only 23 points are used as support vectors, labelled in red.}}{2}\protected@file@percent }
\newlabel{fig:uiregress}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A third degree polynomial least squares regression fitted to a data wave of 46 points with parameters $C=3$ and $e=0.06$. Given these parameters, only 26 points are used as support vectors, labelled in red.}}{2}\protected@file@percent }
\newlabel{fig:uiregressPoly}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Sinc function}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A plot of LS-SVM RBF kernel predictor performance over a range of parameters.}}{3}\protected@file@percent }
\newlabel{fig:crossvalidatedRBFSinc}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A plot of LS-SVM RBF kernel predictor performance over a range of parameters, on the test set.}}{3}\protected@file@percent }
\newlabel{fig:RBFtestsetPerf}{{4}{3}}
\citation{MacKay91bayesianinterpolation}
\citation{MacKay91bayesianinterpolation}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A plot of LS-SVM RBF kernel predictor number five out of all evaluated.}}{4}\protected@file@percent }
\newlabel{fig:fifthModelPlot}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Bayesian tuning}{4}\protected@file@percent }
\citation{wipfNewViewAutomatic}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The most probable model derived using Bayesian inference, with computed error bars.}}{5}\protected@file@percent }
\newlabel{fig:bayesianErrorbars}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Automatic Relevance Determination}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Robust regression}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A noisy sinc dataset.}}{6}\protected@file@percent }
\newlabel{fig:noisyData}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The result of fitting a non-robust LS-SVM model to the noisy sinc data.}}{6}\protected@file@percent }
\newlabel{fig:naiveNonRob}{{8}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Time Series Prediction with Logmap Data}{6}\protected@file@percent }
\bibstyle{acm}
\bibdata{SVMbib}
\bibcite{valyonRobustLSSVMRegression2007}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An LS-SVM regression with a Huber robust loss function on the noisy sinc data.}}{7}\protected@file@percent }
\newlabel{fig:whuberRobust}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The result of applying a non-optimised time series prediction model to the logmap dataset, with the test set in black and the predictions in red.}}{7}\protected@file@percent }
\newlabel{fig:logmapNaive}{{10}{7}}
